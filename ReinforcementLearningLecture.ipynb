{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import sgd\n",
    "import talib\n",
    "import traceback\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning for Trading\n",
    "\n",
    "## What is RL?\n",
    "- Crossover between supervised and unsupervised learning\n",
    "- Solving the problem of learning with delayed reward\n",
    "- A system has a state S\n",
    "- For every state we perform and action based on the state and prior experience\n",
    "- A chain of actions leads to a reward (win/loss)\n",
    "- Every action in the chain can be assigned a fractional reward\n",
    "\n",
    "## Concepts\n",
    "- greedy vs long-term\n",
    "- exploration vs exploitation\n",
    "\n",
    "## Q-Learning\n",
    "- Q-Tables normally based on Markov chains\n",
    "- For each state (x-axis) and each action (y-axis) we get a reward (matrix)\n",
    "- Deep-RL uses neural networks to predict the Q-table\n",
    "- The Q-table is now represented as the weights function in the NN\n",
    "- We update the Q-table at every step based on prior experience\n",
    "- Actions are predicted based on the existing Q-table\n",
    "\n",
    "## Gamification\n",
    "or straight (buy, sell, hold)\n",
    "- State is the screen (technical indicators etc.)\n",
    "- Reward is a win or loss at the end of the game (exit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pickle.load(open('PriceData10.pick','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft['close'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net(df,lkbk,START_IDX,max_mem):\n",
    "    \"\"\"\n",
    "    This initialises the RL run by instantiating a new Game, \n",
    "    creating a new predictive neural network and instantiating\n",
    "    experience replay.\n",
    "    Args:\n",
    "        df: This is the data frame with the market data\n",
    "        lkbk: This is the lookback period, eg. a value of 10 means 10mins, 10hrs and 10days!\n",
    "        START_IDX: This is the starting index for the main loop, allow enough for lkbk\n",
    "        \n",
    "    Returns:\n",
    "        env: an instance of Game, our environment\n",
    "        model: the neural network\n",
    "        exp_replay: an instance of ExperienceReplay\n",
    "    \"\"\"\n",
    "    num_actions = 3\n",
    "    env = Game(df, lkbk=lkbk, init_idx=START_IDX)\n",
    "    hidden_size = len(env.state)*2\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden_size, input_shape=(len(env.state),), activation='relu'))\n",
    "    model.add(Dense(hidden_size, activation='relu'))\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(sgd(lr=.005), \"mse\")\n",
    "    exp_replay = ExperienceReplay(max_memory=max_mem)\n",
    "    return env,model,exp_replay  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "class Game(object):\n",
    "\n",
    "    def __init__(self, df, lkbk=20, max_game_len=1000, init_idx=None):\n",
    "        self.df = df\n",
    "        self.lkbk = lkbk\n",
    "        \n",
    "        self.is_over = False\n",
    "        self.reward = 0\n",
    "        self.pnl_sum = 0\n",
    "        self.init_idx = init_idx\n",
    "        self.reset()\n",
    "        \n",
    "    def _update_state(self, action):\n",
    "        \n",
    "        '''Here we update our state.\n",
    "        The state consists of the current parameters of the system,\n",
    "        similar to the current frame of a gaming screen. It includes\n",
    "        current time, price, position and reward.\n",
    "        We the add secondary features such as technical indicators in _assemble state.\n",
    "        Args:\n",
    "            action: The action suggested by the neural network based on past experience\n",
    "        '''\n",
    "        self.curr_idx += 1\n",
    "        self.curr_time = self.df.index[self.curr_idx]\n",
    "        self.curr_price = self.df['close'][self.curr_idx]\n",
    "        self.pnl = (-self.entry + self.curr_price)*self.position/self.entry\n",
    "        self._assemble_state()\n",
    "        tm_lst = list(map(float,str(self.curr_time.time()).split(':')[:2]))\n",
    "        self._time_of_day = (tm_lst[0]*60 + tm_lst[1])/(24*60) \n",
    "        self._day_of_week  = self.curr_time.weekday()/6\n",
    "        \n",
    "        '''This is where we define our policy and update our position'''\n",
    "        if action == 0:  \n",
    "            pass\n",
    "        \n",
    "        elif action == 2:\n",
    "            \"\"\"---Enter a long or exit a short position---\"\"\"\n",
    "            if self.position == -1:\n",
    "                self.is_over = True\n",
    "                self._get_reward()\n",
    "                self.trade_len = self.curr_idx - self.start_idx\n",
    "   \n",
    "            elif self.position == 0:\n",
    "                self.position = 1\n",
    "                self.entry = self.curr_price\n",
    "                self.start_idx = self.curr_idx\n",
    "            else: \n",
    "                pass\n",
    "            \n",
    "        elif action == 1:\n",
    "            \"\"\"---Enter a short or exit a long position---\"\"\"\n",
    "            if self.position == 1:\n",
    "                self.is_over = True\n",
    "                self._get_reward()\n",
    "                self.trade_len = self.curr_idx - self.start_idx\n",
    "\n",
    "            elif self.position == 0:\n",
    "                self.position = -1\n",
    "                self.entry = self.curr_price\n",
    "                self.start_idx = self.curr_idx\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "    \n",
    "    def _assemble_state(self):\n",
    "        '''Here we can add secondary features such as indicators and times to our current state.\n",
    "        First, we create candlesticks for different bar sizes of 5mins, 1hr and 1d.\n",
    "        We then add some state variables such as time of day, day of week and position.\n",
    "        Next several indicators are added and subsequently z-scored.\n",
    "        '''\n",
    "        \n",
    "        \"\"\"---Adding Candlesticks---\"\"\"\n",
    "        self._get_last_N_timebars()\n",
    "        bars = [self.last5m,self.last1h,self.last1d]\n",
    "        state = []\n",
    "        candles = {j:{k:np.array([]) for k in ['open','high','low','close']} for j in range(len(bars))}\n",
    "        for j,bar in enumerate(bars):\n",
    "            for col in ['open','high','low','close']:\n",
    "                candles[j][col] = np.asarray(bar[col])\n",
    "                state += (list(np.asarray(bar[col]))[-10:])\n",
    "\n",
    "        \"\"\"---Adding State Variables---\"\"\"\n",
    "        self.state = np.array([])\n",
    "        self.state = np.append(self.state,state)\n",
    "        self.state = np.append(self.state,self.position)\n",
    "        np.append(self.state,np.sign(self.pnl_sum))\n",
    "        self.state = np.append(self.state,self._time_of_day)\n",
    "        self.state = np.append(self.state,self._day_of_week)\n",
    "        \n",
    "        \"\"\"---Adding Techincal Indicators---\"\"\"\n",
    "        for c in candles:\n",
    "            try:\n",
    "                sma1 = talib.SMA(candles[c]['close'],self.lkbk-1)[-1]\n",
    "                sma2 = talib.SMA(candles[c]['close'],self.lkbk-8)[-1]\n",
    "                self.state = np.append(self.state,(sma1-sma2)/sma2)\n",
    "                self.state = np.append(self.state,talib.RSI(candles[c]['close'],self.lkbk-1)[-1])\n",
    "                self.state = np.append(self.state,talib.MOM(candles[c]['close'],self.lkbk-1)[-1])\n",
    "                self.state = np.append(self.state,talib.BOP(candles[c]['open'],\n",
    "                                               candles[c]['high'],\n",
    "                                               candles[c]['low'],\n",
    "                                               candles[c]['close'])[-1])\n",
    "\n",
    "                self.state = np.append(self.state,talib.AROONOSC(candles[c]['high'],\n",
    "                                               candles[c]['low'],\n",
    "                                               self.lkbk-3)[-1])\n",
    "            except: print(traceback.format_exc())\n",
    "                \n",
    "        \"\"\"---Z-scoring State---\"\"\"\n",
    "        self.state = (np.array(self.state)-np.mean(self.state,axis=0))/np.std(self.state,axis=0)\n",
    "        \n",
    "    def _get_last_N_timebars(self):\n",
    "        '''This function gets the timebars for the 5m, 1hr and 1d resolution based\n",
    "        on the lookback we've specified.\n",
    "        '''\n",
    "        wdw5m = 9\n",
    "        wdw1h = np.ceil(self.lkbk*15/24.)\n",
    "        wdw1d = np.ceil(self.lkbk*15)\n",
    "        \n",
    "        \"\"\"---creating the candlesticks based on windows---\"\"\"\n",
    "        self.last5m = self.df[self.curr_time-timedelta(wdw5m):self.curr_time].iloc[-self.lkbk:]\n",
    "        self.last1h = self.bars1h[self.curr_time-timedelta(wdw1h):self.curr_time].iloc[-self.lkbk:]\n",
    "        self.last1d = self.bars1d[self.curr_time-timedelta(wdw1d):self.curr_time].iloc[-self.lkbk:]\n",
    "        \n",
    "        '''---Making sure that window lengths agree with lookback---'''\n",
    "        try:\n",
    "            assert(len(self.last5m)==self.lkbk)\n",
    "            assert(len(self.last1h)==self.lkbk)\n",
    "            assert(len(self.last1d)==self.lkbk)\n",
    "        except:\n",
    "            print('****Window length too short****')\n",
    "            print(len(self.last5m),len(self.last1h),len(self.last1d))\n",
    "\n",
    "            self.init_idx = self.curr_idx\n",
    "            self.reset()\n",
    "\n",
    "\n",
    "    def _get_reward(self):\n",
    "        \"\"\"Here we calculate the reward when the game is finished.\n",
    "        Reward function design is very difficult and can significantly\n",
    "        impact the performance of our algo.\n",
    "        In this case we use a simple pnl reward but it is conceivable to use\n",
    "        other metrics such as Sharpe ratio, average return, etc.\n",
    "        \"\"\"\n",
    "        if self.position == 1 and self.is_over:\n",
    "            pnl = (self.entry - self.curr_price)/self.entry\n",
    "            self.reward = pnl\n",
    "        elif self.position == -1 and self.is_over:\n",
    "            pnl = (self.curr_price - self.entry)/self.entry\n",
    "            self.reward = pnl\n",
    "        return self.reward\n",
    "            \n",
    "    def observe(self):\n",
    "        \"\"\"This function returns the state of the system.\n",
    "        Returns:\n",
    "            self.state: the state including indicators, position and times.\n",
    "        \"\"\"\n",
    "        return np.array([self.state])\n",
    "\n",
    "    def act(self, action):\n",
    "        \"\"\"This function updates the state based on an action\n",
    "        that was calculated by the NN.\n",
    "        This is the point where the game interacts with the trading\n",
    "        algo.\n",
    "        \"\"\"\n",
    "        self._update_state(action)\n",
    "        reward = self.reward\n",
    "        game_over = self.is_over\n",
    "        return self.observe(), reward, game_over\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resetting the system for each new trading game.\n",
    "        Here, we also resample the bars for 1h and 1d.\n",
    "        Ideally, we should do this on every update but this will take very long.\n",
    "        \"\"\"\n",
    "        self.pnl = 0\n",
    "        self.entry = 0\n",
    "        self._time_of_day = 0\n",
    "        self._day_of_week = 0\n",
    "        self.curr_idx = self.init_idx         \n",
    "        self.t_in_secs = (self.df.index[-1]-self.df.index[0]).total_seconds()\n",
    "        self.start_idx = self.curr_idx\n",
    "        self.curr_time = self.df.index[self.curr_idx]\n",
    "        self.bars1h = self.df['close'].resample('1H',label='right',closed='right').ohlc().dropna()\n",
    "        self.bars1d = self.df['close'].resample('1D',label='right',closed='right').ohlc().dropna()\n",
    "        self._get_last_N_timebars()\n",
    "        self.state = []\n",
    "        self.position = 0\n",
    "        self._update_state(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay(object):\n",
    "    '''This class calculates the Q-Table.\n",
    "    It gathers memory from previous experience and \n",
    "    creates a Q-Table with states and rewards for each\n",
    "    action using the NN. At the end of the game the reward\n",
    "    is calculated from the reward function. \n",
    "    The weights in the NN are constantly updated with each new\n",
    "    batch of experience. \n",
    "    This is the heart of the RL algorithm.\n",
    "    Args:\n",
    "        state_tp1: state at time t+1\n",
    "        state_t: state at time t\n",
    "        action_t: int {0..2} hold, sell, buy taken at state_t \n",
    "        Q_sa: float, reward for state_tp1\n",
    "        reward_t: reward for state_t\n",
    "        self.memory: list of state_t, action_t and reward_t at time t as well as state_tp1\n",
    "        targets: array(float) Nx2, weight of each action \n",
    "        inputs: an array with scrambled states at different times\n",
    "        targets: Nx3 array of weights for each action for scrambled input states\n",
    "    '''\n",
    "    def __init__(self, max_memory=100, discount=.9):\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def remember(self, states, game_over):\n",
    "        '''Add states to time t and t+1 as well as  to memory'''\n",
    "        self.memory.append([states, game_over])\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def q_table(self, model, batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        num_actions = model.output_shape[-1]\n",
    "        env_dim = self.memory[0][0][0].shape[1]\n",
    "        \n",
    "        \"\"\"---Initialise input and target arrays---\"\"\"\n",
    "        inputs = np.zeros((min(len_memory, batch_size), env_dim))\n",
    "        targets = np.zeros((inputs.shape[0], num_actions))\n",
    "        \n",
    "        \"\"\"Step randomly through different places in the memory\n",
    "        and scramble them into a new input array (inputs) with the\n",
    "        length of the pre-defined batch size\"\"\"\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory, size=inputs.shape[0])):\n",
    "            \n",
    "            \"\"\"Obtain the parameters for Bellman from memory\"\"\"\n",
    "            state_t, action_t, reward_t, state_tp1 = self.memory[idx][0]\n",
    "            game_over = self.memory[idx][1]\n",
    "\n",
    "            inputs[i] = state_t\n",
    "            \n",
    "            \"\"\"---Calculate the targets for the state at time t---\"\"\"\n",
    "            targets[i] = model.predict(state_t)[0]\n",
    "            \n",
    "            \"\"\"---Calculate the reward at time t+1 for action at time t---\"\"\"\n",
    "            Q_sa = np.max(model.predict(state_tp1)[0])\n",
    "           \n",
    "            if game_over:\n",
    "                \"\"\"---When game is over we have a definite reward---\"\"\"\n",
    "                targets[i, action_t] = reward_t\n",
    "            else:\n",
    "                \"\"\"\n",
    "                ---Update the part of the target for which action_t occured to new value---\n",
    "                Q_new(s,a) = (1-gamma) * reward_t + gamma * max_a' Q(s', a')\n",
    "                \"\"\"\n",
    "                targets[i, action_t] = (1-self.discount)*reward_t + self.discount * Q_sa\n",
    "        \n",
    "      \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run(df):\n",
    "\n",
    "    epsilon_0 = .001\n",
    "    max_mem = 600\n",
    "    max_trade_time = 1000\n",
    "    batch_size = 500\n",
    "    lkbk = 20\n",
    "    START_IDX = 3000\n",
    "    pnls = []\n",
    "    pnl_dates = []\n",
    "    e = 0\n",
    "\n",
    "    \"\"\"---Initialise a NN and a set up initial game parameters---\"\"\"\n",
    "    env,model,exp_replay = init_net(df,lkbk,START_IDX,max_mem)\n",
    "\n",
    "    \"\"\"---Loop that steps through one trade (game) at a time---\"\"\"\n",
    "    while True:\n",
    "        \n",
    "        \"\"\"---Stop the algo when end is near to avoid exception---\"\"\"\n",
    "        if env.curr_idx >= len(df)-max_trade_time:\n",
    "            break\n",
    "        \n",
    "        e += 1\n",
    "        \n",
    "        \"\"\"---Calculate epsilon for exploration vs exploitation random action generator---\"\"\"\n",
    "        epsilon = epsilon_0**(np.log10(e))+0.001\n",
    "         \n",
    "        \"\"\"---Initialise a new game---\"\"\"\n",
    "        env = Game(df, lkbk=lkbk, init_idx=env.curr_idx)\n",
    "        game_over = False\n",
    "        state_tp1 = env.observe()\n",
    "        cnt = 0\n",
    "        \n",
    "        \"\"\"---Walk through time steps starting from the end of the last game---\"\"\"\n",
    "        while not game_over:\n",
    "            cnt += 1\n",
    "            state_t = state_tp1\n",
    "            \n",
    "            #\"\"\"---Generate a random action---\"\"\"\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = np.random.randint(0, 3, size=1)[0]\n",
    "                if env.position == 0:\n",
    "                    if action == 2:\n",
    "                        exit_action = 1\n",
    "                    elif action == 1:\n",
    "                        exit_action = 2\n",
    "\n",
    "            \n",
    "            #\"\"\"---Action for opening a trade---\"\"\"\n",
    "            elif env.position == 0:\n",
    "                q = model.predict(state_t)\n",
    "                action = np.argmax(q[0])\n",
    "                if action:\n",
    "                    exit_action = np.argmin(q[0][1:])+1\n",
    "            \n",
    "            #\"\"\"---Time Exit---\"\"\"\n",
    "            elif cnt > max_trade_time:\n",
    "                action = exit_action\n",
    "                \n",
    "            #\"\"\"---Action for closing a trade---\"\"\"\n",
    "            elif env.position:\n",
    "                q = model.predict(state_t)\n",
    "                action = np.argmax(q[0])\n",
    "\n",
    "            #\"\"\"---Updating the Game---\"\"\"\n",
    "            state_tp1, reward, game_over = env.act(action)\n",
    "            \n",
    "            #\"\"\"---Adding state to memory---\"\"\"\n",
    "            exp_replay.remember([state_t, action, reward, state_tp1], game_over)\n",
    "\n",
    "            #\"\"\"---Creating a new Q-Table---\"\"\"\n",
    "            inputs, targets = exp_replay.q_table(model, batch_size=batch_size)\n",
    "            env.pnl_sum = sum(pnls)\n",
    "\n",
    "            #\"\"\"---Update the NN model with a new Q-Table\"\"\"\n",
    "            #if not env.curr_idx%50 or env.curr_idx<3500:\n",
    "            loss = model.train_on_batch(inputs, targets)\n",
    "            \n",
    "        print(\"Trade {:03d} | pos {} | len {} | pnl {:,.2f}% | eps {:,.4f} | {} | {}\".format(e,  \n",
    "                                                                              env.position, \n",
    "                                                                              env.trade_len,\n",
    "                                                                              sum(pnls)*100,\n",
    "                                                                              epsilon,\n",
    "                                                                              env.curr_time,\n",
    "                                                                              env.curr_idx))\n",
    "\n",
    "        pnls.append(env.pnl)\n",
    "        pnl_dates.append(env.curr_time)\n",
    "        pickle.dump([pnl_dates,pnls],open('pnls.pick','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(34)\n",
    "run(dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates,pnls=pickle.load(open('pnls_ma3_98-11.pick','rb'))\n",
    "\n",
    "plt.plot(dates,np.cumsum(pnls))\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
